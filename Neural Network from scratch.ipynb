{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "303aae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "2f344294",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, num_inputs=4, num_hidden=[5,6], num_output=2):\n",
    "        \n",
    "        \n",
    "        #### initialize layers ###\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden \n",
    "        self.num_output = num_output\n",
    "        \n",
    "        self.layers = [self.num_inputs] + self.num_hidden + [self.num_output]\n",
    "        \n",
    "        ### initialize weights ###\n",
    "        \n",
    "        self.weights = []\n",
    "        for i in range(0,len(self.layers)-1): #no weights for input layer\n",
    "            weight = np.random.rand(self.layers[i], self.layers[i+1])\n",
    "            self.weights.append(weight)\n",
    "        \n",
    "        \n",
    "        ### initialize Activations ###\n",
    "        \n",
    "        self.activations = []\n",
    "        for i in range(len(self.layers)):\n",
    "            temp_zeros = np.zeros(self.layers[i])\n",
    "            self.activations.append(temp_zeros)\n",
    "        \n",
    "        ### initialize Derivatives ###\n",
    "        \n",
    "        self.derivatives = []\n",
    "        for i in range(len(self.layers)-1):\n",
    "            temp_zeros = np.zeros((self.layers[i], self.layers[i+1]))\n",
    "            self.derivatives.append(temp_zeros)\n",
    "        \n",
    "        self.bias_derivative = []\n",
    "        for i in range(len(self.layers)-1):\n",
    "            temp_zeros = np.zeros((self.layers[i+1]))\n",
    "            self.bias_derivative.append(temp_zeros)\n",
    "        \n",
    "        self.bias = []\n",
    "        for i in range(len(self.layers)-1):\n",
    "            b = np.random.rand(self.layers[i+1])\n",
    "            self.bias.append(b)\n",
    "            \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-1*x))\n",
    "    \n",
    "    def sigmoid_derivative(self,sigx):\n",
    "        return sigx*(1.0-sigx)\n",
    "    \n",
    "    def Relu(self, x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    def Relu_derivative(self,relux):\n",
    "        res = []\n",
    "        for ele in relux:\n",
    "            res.append(0) if ele==0 else res.append(1)\n",
    "        return res\n",
    "    \n",
    "    def forward_propagation(self,inputs, activation='Relu'):\n",
    "        activations = inputs\n",
    "        self.activations[0] = inputs\n",
    "        \n",
    "        #iterate throuugh wi\n",
    "        for i,weight in enumerate(self.weights):\n",
    "            \n",
    "            #calculate each layer input (hi)\n",
    "            net_inputs = np.dot(activations, weight) + self.bias[i]\n",
    "            \n",
    "            if i == len(self.weights)-1:\n",
    "                break\n",
    "            if activation == 'Sigmoid':\n",
    "                #Calculate the activations (ai)\n",
    "                activations = self.sigmoid(net_inputs)\n",
    "\n",
    "            elif activation == 'Relu':\n",
    "                activations = self.Relu(net_inputs)\n",
    "                \n",
    "            self.activations[i+1] = activations\n",
    "            \n",
    "        activations = self.sigmoid(net_inputs)   \n",
    "        self.activations[len(self.weights)] = activations\n",
    "        return activations\n",
    "    \n",
    "    def back_propagation(self, error, activation = 'Relu'):\n",
    "        \n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            ## get Sigmoid(h[i+1]) = activations[i+1]\n",
    "            activations = self.activations[i+1] \n",
    "            \n",
    "            if i == len(self.derivatives)-1 or activation == 'Sigmoid': \n",
    "                ## delta = (a[i+1] - y) * sigmoid'(h[i+1])\n",
    "                delta = error * self.sigmoid_derivative(activations)\n",
    "            else:\n",
    "                ## delta = (a[i+1] - y) * sigmoid'(h[i+1])\n",
    "                delta = error * self.Relu_derivative(activations)\n",
    "\n",
    "            ## calculate error derivative of current layer ## (delta E/ delta Wi) \n",
    "            current_act = self.activations[i]\n",
    "            \n",
    "            #transform current_act into 3x1 matrix (layers[i]x1)\n",
    "            current_act = current_act.reshape(current_act.shape[0], -1)  #-1 tells to automatically calculate the value of the column\n",
    "            \n",
    "            #transform delta into 1X2 matrix (1xlayers[i+1])\n",
    "            delta = delta.reshape(1,len(delta))\n",
    "            \n",
    "            ## (delta E/ delta Wi) ##\n",
    "            self.derivatives[i] = np.dot(current_act, delta) #(layers[i]xlayers[i+1])         \n",
    "            self.bias_derivative[i] = delta\n",
    "            \n",
    "            ## calculate new error ##\n",
    "            error = np.dot(delta,self.weights[i].T).flatten()\n",
    "            \n",
    "\n",
    "        return error\n",
    "    \n",
    "    \n",
    "    def gradient_decent(self, learning_rate = 0.5):\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            weight = self.weights[i]\n",
    "            bias = self.bias[i]\n",
    "            derivative = self.derivatives[i]\n",
    "            weight = weight + learning_rate * derivative\n",
    "            bias = bias + learning_rate* self.bias_derivative[i]\n",
    "            self.weights[i] = weight\n",
    "    \n",
    "    def train(self,inputs, targets, epochs = 50, learning_rate = 0.2, verbose = False, activation = 'Relu'):\n",
    "        for i in range(0,epochs):   \n",
    "            sum_error = 0\n",
    "            if verbose:\n",
    "                print(\"\\n-------------Epoch: {}----------------\\n\".format(i+1))\n",
    "            \n",
    "            for (input, target) in zip(inputs,targets):\n",
    "                #forward prop\n",
    "                output = self.forward_propagation(input, activation = activation)\n",
    "\n",
    "                #calculate error\n",
    "                error = target - output\n",
    "                \n",
    "                sum_error +=self.mse(target, output)\n",
    "                #sum_error += abs(error)\n",
    "                \n",
    "                #back Prop\n",
    "                error = self.back_propagation(error, activation = 'Relu')\n",
    "                \n",
    "                #gradient descent\n",
    "                self.gradient_decent(learning_rate)\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"-------------ERROR: {}----------------\\n\\n\".format(sum_error.round(5)))            \n",
    "    \n",
    "    def sign(self,x):\n",
    "        return 0 if x<0 else 1    \n",
    "    \n",
    "    def mse(self,target, output):\n",
    "        sum = 0\n",
    "        for i in range(len(target)):\n",
    "            sum+= (target[i]-output[i])**2\n",
    "        return sum/2\n",
    "\n",
    "\n",
    "    def regression_predict(self,input):\n",
    "        output=self.forward_propagation(input)\n",
    "        print(output)\n",
    "        return output\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "5c913129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Classifier:\n",
    "    def __init__(self, num_inputs=4, num_hidden=[5,6], num_output=2):\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden \n",
    "        self.num_output = num_output\n",
    "        \n",
    "        self.layers = [self.num_inputs] + self.num_hidden + [self.num_output]\n",
    "        \n",
    "        ### initialize weights ###\n",
    "        \n",
    "        self.weights = []\n",
    "        for i in range(0,len(self.layers)-1): #no weights for input layer\n",
    "            weight = np.random.rand(self.layers[i], self.layers[i+1])\n",
    "            self.weights.append(weight)\n",
    "        \n",
    "        \n",
    "        ### initialize Activations ###\n",
    "        \n",
    "        self.activations = []\n",
    "        for i in range(len(self.layers)):\n",
    "            temp_zeros = np.zeros(self.layers[i])\n",
    "            self.activations.append(temp_zeros)\n",
    "        \n",
    "        ### initialize Derivatives ###\n",
    "        \n",
    "        self.derivatives = []\n",
    "        for i in range(len(self.layers)-1):\n",
    "            temp_zeros = np.zeros((self.layers[i], self.layers[i+1]))\n",
    "            self.derivatives.append(temp_zeros)\n",
    "        \n",
    "        self.bias_derivative = []\n",
    "        for i in range(len(self.layers)-1):\n",
    "            temp_zeros = np.zeros((self.layers[i+1]))\n",
    "            self.bias_derivative.append(temp_zeros)\n",
    "        \n",
    "        self.bias = []\n",
    "        for i in range(len(self.layers)-1):\n",
    "            b = np.random.rand(self.layers[i+1])\n",
    "            self.bias.append(b)\n",
    "            \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-1*x))\n",
    "    \n",
    "    def sigmoid_derivative(self,sigx):\n",
    "        return sigx*(1.0-sigx)\n",
    "    \n",
    "    def Relu(self, x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    def Relu_derivative(self,relux):\n",
    "        res = []\n",
    "        for ele in relux:\n",
    "            res.append(0) if ele==0 else res.append(1)\n",
    "        return res\n",
    "    \n",
    "    def forward_propagation(self,inputs, activation='Relu'):\n",
    "        activations = inputs\n",
    "        self.activations[0] = inputs\n",
    "        \n",
    "        #iterate throuugh wi\n",
    "        for i,weight in enumerate(self.weights):\n",
    "            \n",
    "            #calculate each layer input (hi)\n",
    "            net_inputs = np.dot(activations, weight) + self.bias[i]\n",
    "            \n",
    "            if i == len(self.weights)-1:\n",
    "                break\n",
    "            if activation == 'Sigmoid':\n",
    "                #Calculate the activations (ai)\n",
    "                activations = self.sigmoid(net_inputs)\n",
    "\n",
    "            elif activation == 'Relu':\n",
    "                activations = self.Relu(net_inputs)   \n",
    "            self.activations[i+1] = activations\n",
    "            \n",
    "        activations = self.sigmoid(net_inputs)   \n",
    "        self.activations[len(self.weights)] = activations\n",
    "        #print(self.activations[1:], net_inputs)\n",
    "        return activations\n",
    "    \n",
    "    def back_propagation(self, error, activation = 'Relu'):\n",
    "        \n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            ## get Sigmoid(h[i+1]) = activations[i+1]\n",
    "            activations = self.activations[i+1] \n",
    "            \n",
    "            if i == len(self.derivatives)-1 or activation == 'Sigmoid': \n",
    "                ## delta = (a[i+1] - y) * sigmoid'(h[i+1])\n",
    "                delta = error * self.sigmoid_derivative(activations)\n",
    "            else:\n",
    "                ## delta = (a[i+1] - y) * sigmoid'(h[i+1])\n",
    "                delta = error * self.Relu_derivative(activations)\n",
    "\n",
    "            ## calculate error derivative of current layer ## (delta E/ delta Wi) \n",
    "            current_act = self.activations[i]\n",
    "            #\n",
    "            #transform current_act into 3x1 matrix (layers[i]x1)\n",
    "            current_act = current_act.reshape(current_act.shape[0], -1)  #-1 tells to automatically calculate the value of the column\n",
    "            \n",
    "            #transform delta into 1X2 matrix (1xlayers[i+1])\n",
    "            delta = delta.reshape(1,len(delta))\n",
    "            \n",
    "            ## (delta E/ delta Wi) ##\n",
    "            self.derivatives[i] = np.dot(current_act, delta) #(layers[i]xlayers[i+1])         \n",
    "            self.bias_derivative[i] = delta\n",
    "            \n",
    "            ## calculate new error ##\n",
    "            error = np.dot(delta,self.weights[i].T).flatten()\n",
    "            \n",
    "\n",
    "        return error\n",
    "    \n",
    "    \n",
    "    def gradient_decent(self, learning_rate = 0.5):\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            weight = self.weights[i]\n",
    "            bias = self.bias[i]\n",
    "            derivative = self.derivatives[i]\n",
    "            weight = weight + learning_rate * derivative\n",
    "            bias = bias + learning_rate* self.bias_derivative[i]\n",
    "            self.weights[i] = weight\n",
    "    \n",
    "    def train(self,inputs, targets, epochs = 50, learning_rate = 0.2, verbose = False, activation = 'Relu'):\n",
    "        for i in range(0,epochs):   \n",
    "            sum_error = 0\n",
    "            if verbose:\n",
    "                print(\"\\n-------------Epoch: {}----------------\\n\".format(i+1))\n",
    "            \n",
    "            for (input, target) in zip(inputs,targets):\n",
    "                #forward prop\n",
    "                output = self.forward_propagation(input, activation = activation)\n",
    "\n",
    "                #calculate error\n",
    "                error = target - output\n",
    "                \n",
    "                sum_error += target - output\n",
    "                #sum_error += abs(error)\n",
    "                \n",
    "                #back Prop\n",
    "                error = self.back_propagation(error, activation = 'Relu')\n",
    "                \n",
    "                #gradient descent\n",
    "                self.gradient_decent(learning_rate)\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"-------------ERROR: {}----------------\\n\\n\".format(sum_error.round(5)/inputs.shape[1]))            \n",
    "\n",
    "    def classify_predict(self,input):\n",
    "        output = self.forward_propagation(input)\n",
    "        pred = np.where(output == max(output))[0]\n",
    "        return pred\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "5c775d5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp = MLP(2,[5],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "8b564842",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[random()/2 for _ in range(2)] for _ in range(800)])\n",
    "target = np.array([[i[0] + i[1]] for i in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "6545a0f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------Epoch: 1----------------\n",
      "\n",
      "-------------ERROR: 40.40573----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 2----------------\n",
      "\n",
      "-------------ERROR: 17.23148----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 3----------------\n",
      "\n",
      "-------------ERROR: 15.52494----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 4----------------\n",
      "\n",
      "-------------ERROR: 13.40937----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 5----------------\n",
      "\n",
      "-------------ERROR: 10.96942----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 6----------------\n",
      "\n",
      "-------------ERROR: 8.43844----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 7----------------\n",
      "\n",
      "-------------ERROR: 6.11566----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 8----------------\n",
      "\n",
      "-------------ERROR: 4.22229----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 9----------------\n",
      "\n",
      "-------------ERROR: 2.82434----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 10----------------\n",
      "\n",
      "-------------ERROR: 1.864----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 11----------------\n",
      "\n",
      "-------------ERROR: 1.23347----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 12----------------\n",
      "\n",
      "-------------ERROR: 0.8288----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 13----------------\n",
      "\n",
      "-------------ERROR: 0.57081----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 14----------------\n",
      "\n",
      "-------------ERROR: 0.40583----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 15----------------\n",
      "\n",
      "-------------ERROR: 0.2995----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 16----------------\n",
      "\n",
      "-------------ERROR: 0.23036----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 17----------------\n",
      "\n",
      "-------------ERROR: 0.185----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 18----------------\n",
      "\n",
      "-------------ERROR: 0.15502----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 19----------------\n",
      "\n",
      "-------------ERROR: 0.13508----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 20----------------\n",
      "\n",
      "-------------ERROR: 0.12174----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 21----------------\n",
      "\n",
      "-------------ERROR: 0.11278----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 22----------------\n",
      "\n",
      "-------------ERROR: 0.10674----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 23----------------\n",
      "\n",
      "-------------ERROR: 0.10266----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 24----------------\n",
      "\n",
      "-------------ERROR: 0.09989----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 25----------------\n",
      "\n",
      "-------------ERROR: 0.09801----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 26----------------\n",
      "\n",
      "-------------ERROR: 0.09674----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 27----------------\n",
      "\n",
      "-------------ERROR: 0.09587----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 28----------------\n",
      "\n",
      "-------------ERROR: 0.09527----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 29----------------\n",
      "\n",
      "-------------ERROR: 0.09487----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 30----------------\n",
      "\n",
      "-------------ERROR: 0.09459----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 31----------------\n",
      "\n",
      "-------------ERROR: 0.09441----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 32----------------\n",
      "\n",
      "-------------ERROR: 0.09428----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 33----------------\n",
      "\n",
      "-------------ERROR: 0.09419----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 34----------------\n",
      "\n",
      "-------------ERROR: 0.09413----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 35----------------\n",
      "\n",
      "-------------ERROR: 0.09409----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 36----------------\n",
      "\n",
      "-------------ERROR: 0.09406----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 37----------------\n",
      "\n",
      "-------------ERROR: 0.09404----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 38----------------\n",
      "\n",
      "-------------ERROR: 0.09403----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 39----------------\n",
      "\n",
      "-------------ERROR: 0.09402----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 40----------------\n",
      "\n",
      "-------------ERROR: 0.09401----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 41----------------\n",
      "\n",
      "-------------ERROR: 0.09401----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 42----------------\n",
      "\n",
      "-------------ERROR: 0.09401----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 43----------------\n",
      "\n",
      "-------------ERROR: 0.094----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 44----------------\n",
      "\n",
      "-------------ERROR: 0.094----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 45----------------\n",
      "\n",
      "-------------ERROR: 0.094----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 46----------------\n",
      "\n",
      "-------------ERROR: 0.094----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 47----------------\n",
      "\n",
      "-------------ERROR: 0.094----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 48----------------\n",
      "\n",
      "-------------ERROR: 0.094----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 49----------------\n",
      "\n",
      "-------------ERROR: 0.094----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 50----------------\n",
      "\n",
      "-------------ERROR: 0.094----------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp.train(inputs,target, 50, learning_rate = 0.05, verbose = True, activation = 'Relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "cd55c228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17364382, 0.31901729])"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "b1ff6eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49266112])"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "b874b8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49179673]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.49179673])"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.regression_predict(inputs[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "6005dc15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50036645]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.50036645])"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.regression_predict([0.1,0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "8c91a0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38857433]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.38857433])"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.regression_predict([0.1,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "e94ab764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16934932]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.16934932])"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.regression_predict([0.1,0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58f8a2",
   "metadata": {},
   "source": [
    "## Mobile Phone Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "11648910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\rvikr\\CEG ML Lab\\train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "69818314",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['label'].isin([1,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "f5260121",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_org = df.drop(['label'], axis=1)\n",
    "target = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "5f96b097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8816, 784)"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_org.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "73f486a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(X):\n",
    "    unique_values = list(set(X))\n",
    "    unique_values.sort()\n",
    "    res = [[1 if num == value else 0 for value in unique_values] for num in X]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "0373d606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = one_hot_encode(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "2418cb63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 2)"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "071fed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "a6a42077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8816, 2)"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "b45f1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_org = inputs_org.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "ba54f99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs_org/255\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "dfd01dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP_Classifier(784,[16],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "2a76accf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------Epoch: 1----------------\n",
      "\n",
      "-------------ERROR: [-0.56697522 -0.50467772]----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 2----------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rvikr\\AppData\\Local\\Temp\\ipykernel_2096\\1022464144.py:43: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-1*x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------ERROR: [ 0.04464432 -0.04491029]----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 3----------------\n",
      "\n",
      "-------------ERROR: [ 0.0321836  -0.03150974]----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 4----------------\n",
      "\n",
      "-------------ERROR: [ 0.02826784 -0.03022084]----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 5----------------\n",
      "\n",
      "-------------ERROR: [ 0.02895786 -0.02847612]----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 6----------------\n",
      "\n",
      "-------------ERROR: [ 0.03447658 -0.03549278]----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 7----------------\n",
      "\n",
      "-------------ERROR: [ 0.02513959 -0.02839131]----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 8----------------\n",
      "\n",
      "-------------ERROR: [ 0.03448802 -0.0370871 ]----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 9----------------\n",
      "\n",
      "-------------ERROR: [ 0.01772871 -0.0184068 ]----------------\n",
      "\n",
      "\n",
      "\n",
      "-------------Epoch: 10----------------\n",
      "\n",
      "-------------ERROR: [ 0.03579139 -0.03353258]----------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp.train(inputs,targets,10, 0.5,True, activation = 'Sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "a6ffc9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = mlp.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "30946cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = mlp.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "8cb2e514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_org[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "2c6b5ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rvikr\\AppData\\Local\\Temp\\ipykernel_2096\\1022464144.py:43: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-1*x))\n"
     ]
    }
   ],
   "source": [
    "## Test Images ##\n",
    "images = []\n",
    "predictions = []\n",
    "for i in range(0,5):\n",
    "    images.append(inputs_org[i*55].reshape(28, 28))\n",
    "    predictions.append(mlp.classify_predict(inputs[i*55]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "0a3a6e35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(10, 10))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through the images and plot them\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(images[i], cmap='gray') \n",
    "    ax.axis('off') \n",
    "    ax.set_title('Predicted Class: {}'.format(predictions[i]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
